{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device agnoistic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path(\"./data/\")\n",
    "image_path = data_path / \"CT-KIDNEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunctions import walk_through_dir \n",
    "image_path = data_path / \"CT-KIDNEY-VAL\"\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/CT-KIDNEY-VAL/train'),\n",
       " WindowsPath('data/CT-KIDNEY-VAL/test'),\n",
       " WindowsPath('data/CT-KIDNEY-VAL/val'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup train and testing paths \n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "val_dir = image_path / \"val\"\n",
    "\n",
    "train_dir, test_dir, val_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\CT-KIDNEY-VAL\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mtest_dir, \n\u001b[0;32m      4\u001b[0m                                 transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m      5\u001b[0m val_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mval_dir, \n\u001b[0;32m      6\u001b[0m                                 transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data\\\\CT-KIDNEY-VAL\\\\train'"
     ]
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(root= train_dir, \n",
    "                                transform= transform)\n",
    "test_data = datasets.ImageFolder(root=test_dir, \n",
    "                                transform=transform)\n",
    "val_data = datasets.ImageFolder(root=val_dir, \n",
    "                                transform=transform)\n",
    "train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=os.cpu_count(),\n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             shuffle=False)\n",
    "val_dataloader = DataLoader(dataset=val_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             shuffle=False)\n",
    "train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images \n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image and label from the batch \n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shape \n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.axis(False)\n",
    "plt.title(class_names[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit from Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example values \n",
    "height = 224 \n",
    "width = 224 \n",
    "color_channels = 3\n",
    "patch_size = 16\n",
    "\n",
    "# Calculate number of patches \n",
    "number_of_patches = int((height*width)/patch_size**2)\n",
    "number_of_patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape \n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "# Output shape \n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2*color_channels)\n",
    "embedding_layer_input_shape, embedding_layer_output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning a single image into patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a single image \n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_permuted = image.permute(1,2,0)\n",
    "patch_size = 16 \n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224 \n",
    "patch_size = 16 \n",
    "num_patches = img_size/patch_size \n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
    "print(f'Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels')\n",
    "# Create a series of subplot \n",
    "fig, axs = plt.subplots(nrows=1,\n",
    "                        ncols=img_size // patch_size, # one column for each patch\n",
    "                        sharex=True,\n",
    "                        sharey=True,\n",
    "                        figsize=(patch_size, patch_size))\n",
    "# Iterate through number of patches in top row \n",
    "for i, patch in enumerate(range(0,img_size,patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size,:])\n",
    "    axs[i].set_xlabel(i+1)\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code to plot whole image as patch\n",
    "img_size = 224 \n",
    "patch_size = 16 \n",
    "num_patches = img_size/patch_size \n",
    "assert img_size%patch_size==0, \"Image size must be divisible by patch size.\"\n",
    "print(f'''Number of patches per row: {num_patches}\n",
    "Number of patches per column: {num_patches}\n",
    "Total patches: {num_patches*num_patches}\n",
    "Patch size: {patch_size} pixels x {patch_size} pixels''')\n",
    "\n",
    "# Create a series of subplots \n",
    "fig, axs = plt.subplots(nrows=img_size//patch_size,\n",
    "                        ncols=img_size//patch_size,\n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "# Loop through height and width \n",
    "for i, patch_height in enumerate(range(0,img_size,patch_size)):\n",
    "    for j, patch_width in enumerate(range(0,img_size, patch_size)):\n",
    "        # Plot the permuted image on different axis \n",
    "        axs[i,j].imshow(image_permuted[patch_height:patch_height+patch_size,\n",
    "                                       patch_width:patch_width+patch_size, \n",
    "                                       :])\n",
    "        axs[i,j].set_ylabel(i+1,\n",
    "                            rotation=\"horizontal\",\n",
    "                            horizontalalignment='right',\n",
    "                            verticalalignment='center')\n",
    "        axs[i,j].set_xlabel(j+1)\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "        axs[i,j].label_outer()\n",
    "fig.suptitle(f'{class_names[label]} -> Patchified', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  \n",
    "# Set the patch size \n",
    "patch_size = 16 \n",
    "# Create a conv2d layer with hyperparameters from ViT paper \n",
    "conv2d = nn.Conv2d(in_channels=3, \n",
    "                   out_channels=768, # D size from table 1 \n",
    "                   kernel_size=patch_size,\n",
    "                   stride=patch_size,\n",
    "                   padding=0)\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) \n",
    "image_out_of_conv.shape, image_out_of_conv.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random_indexs = random.sample(range(0,758),k=5)\n",
    "print(f'Showing random convolutional feature maps from indexes: {random_indexs}')\n",
    "\n",
    "# Create plot \n",
    "fig, axs = plt.subplots(nrows=1,ncols=5)\n",
    "\n",
    "for i, idx in enumerate(random_indexs):\n",
    "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] \n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())# squueze->removes batch dimension, detach -> removed grads, numpy -> turns to the numpy array \n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = nn.Flatten(start_dim=2,\n",
    "                           end_dim=3)\n",
    "image_out_of_conv_flattened = flatten_layer(image_out_of_conv)\n",
    "image_out_of_conv_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Image feature map (patches) shape: {image_out_of_conv.shape}')\n",
    "print(f'Flattened image feature map shape: {image_out_of_conv_flattened.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0,2,1)\n",
    "single_flattened_feature_map = image_out_of_conv_flattened_permuted[:, :, 0]\n",
    "plt.figure(figsize=(22,22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f'Flattened feature map shape: {single_flattened_feature_map.shape}')\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation 1: Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution%patch_size==0, f\"Imput image size must be divisible by patch size, image shape: {image_resolution}, potch size: {self.patch_size}\"\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        return x_flattened.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "print(f'Input image size: {image.unsqueeze(0).shape}')\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) \n",
    "print(f'Output patch embedding sequeence shape: {patch_embedded_image.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "batch_size, embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True)\n",
    "class_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
    "                                                       dim=1)\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f'Sequence of the patch embeddings with class token prepend shape: {patch_embedded_image_with_class_embedding.shape} -> (bathc_size, class_token + number_of_patchs, embedding_dim)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_patches = int((height*width) / patch_size**2)\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[-1]\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True)\n",
    "position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the position embedding to the patch and class token embedding \n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "print(patch_and_position_embedding)\n",
    "print(f'patch and position embedding shape: {patch_and_position_embedding.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation 2: Multihead Self-Attention (MSA Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dimension:int=768, \n",
    "                 num_heads:int=12, \n",
    "                 attn_dropout:int=0):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dimension)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dimension, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_self_attention_block = MultiHeadSelfAttentionBlock(embedding_dimension=768,\n",
    "                                                             num_heads=12,\n",
    "                                                             attn_dropout=0)\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f'Input shape of MSA block: {patch_and_position_embedding.shape}')\n",
    "print(f'Output shape of MSA block: {patched_image_through_msa_block.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation 3: MultiLayer Perceptron block\n",
    "\n",
    "```python\n",
    "#MLP \n",
    "x = Linear -> non-linear -> droput -> linear -> dropout\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 dropout:int=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                    out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                    out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = MLPBlock(embedding_dim=768,\n",
    "                     mlp_size=3072,\n",
    "                     dropout=0.1)\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f'Input shape of MLP block: {patched_image_through_msa_block.shape}')\n",
    "print(f'Output shape of MLP block: {patched_image_through_mlp_block.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, \n",
    "                 num_heads:int=12, \n",
    "                 mlp_size:int=3072, \n",
    "                 mlp_dropout:float=0.1, \n",
    "                 attn_dropout:float=0): \n",
    "        super().__init__()\n",
    "        self.msa_block = MultiHeadSelfAttentionBlock(embedding_dimension=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "    def forward(self, x):\n",
    "        x =  self.msa_block(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 img_size:int=224,\n",
    "                 in_channels:int=3, \n",
    "                 patch_size:int=16,\n",
    "                 num_transformer_layers:int=12,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:int=0,\n",
    "                 mlp_dropout:int=0.1,\n",
    "                 embedding_dropout:int=0.1, \n",
    "                 num_classes:int=1000): \n",
    "        super().__init__()\n",
    "        assert img_size%patch_size==0, f'Image size must be divisible by patch size, image: {img_size}, patch size: {patch_width}'\n",
    "        self.num_patches=(img_size*img_size) // patch_size**2\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1,1,embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1,self.num_patches+1, embedding_dim))\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                           num_heads=num_heads,\n",
    "                                                                           mlp_size=mlp_size,\n",
    "                                                                           mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )  \n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = x.to(next(self.parameters()).device)\n",
    "        batch_size = x.shape[0]\n",
    "        class_token = self.class_embedding.expand(batch_size,-1,-1)\n",
    "        x = self.patch_embedding(x)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        x = self.position_embedding + x\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:,0])\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "random_image_tensor = torch.randn(1,3,224,224)\n",
    "vit = ViT(num_classes=len(class_names)).to(device)\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=ViT(num_classes=len(class_names)),\n",
    "        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vit.parameters(), \n",
    "                             lr=1e-3,\n",
    "                             betas=(0.9,0.999),\n",
    "                             weight_decay=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_train import train\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "results = train(model=vit,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    test_dataloader=val_dataloader,\n",
    "                    epochs=30,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of rsults dictionary\"\"\"\n",
    "    # Get the loss values of results dictionary (training and test)\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss= results[\"test_loss\"]\n",
    "    # Get the accuracy values of the results dictionary (training and val)\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "    # Figure out number of epochs \n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15,7))\n",
    "    # Plot the loss \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the accuracy \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_acc\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_acc\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "vit.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    " \n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained ViT from `torchvision.models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad=False\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "pretrained_vit.heads = nn.Linear(in_features=768,\n",
    "                                 out_features=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=pretrained_vit,\n",
    "        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for the pretrained ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "pretrained_vit_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "# Image folders\n",
    "train_data_pretrained = datasets.ImageFolder(root= train_dir, \n",
    "                                transform= transform)\n",
    "test_data_pretrained = datasets.ImageFolder(root=test_dir, \n",
    "                                transform=transform)\n",
    "val_data_pretrained = datasets.ImageFolder(root=val_dir, \n",
    "                                transform=transform)\n",
    "# Data Loaders\n",
    "train_dataloader_pretrained = DataLoader(dataset=train_data_pretrained,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=os.cpu_count(),\n",
    "                              shuffle=True)\n",
    "test_dataloader_pretrained = DataLoader(dataset=test_data_pretrained,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             shuffle=False)\n",
    "val_dataloader_pretrained = DataLoader(dataset=val_data_pretrained,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=os.cpu_count(),\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data_pretrained.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "pretrained_vit_results = train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=val_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=10,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "import seaborn as sns\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "pretrained_vit.eval() \n",
    "with torch.inference_mode():\n",
    "    for images, labels in test_dataloader_pretrained:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = pretrained_vit(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(pretrained_vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "# Create model dictory path\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True,\n",
    "                exist_ok=True)\n",
    "# Create model save \n",
    "MODEL_NAME = \"vit.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "# Save the model state dict \n",
    "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=vit.state_dict(),\n",
    "          f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vit_pretrained.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
    "# Save the model state dict \n",
    "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=vit.state_dict(),\n",
    "          f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_path = data_path / \"Tumor1.jpg\"\n",
    "custom_image_uint8 = torchvision.io.read_image(custom_image_path)\n",
    "print(f\" Custom image tensor: \\n{custom_image_uint8}\")\n",
    "print(f\" Custom image shape: {custom_image_uint8.shape}\")\n",
    "print(f\" Custom image dtype: {custom_image_uint8.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunctions import pred_and_plot_image\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    transform = custom_transform,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
